---
title: PAC framework
date: 2019-09-29 00:00:00
mathjax: true
categories:
  - Machine Learning
author_staff_member: viktor-remeli
image: "https://unsplash.it/600/450?image=777&a=.png"
large_header: false
---

*Intro about the PAC framework.*

### **Probably Approximately Correct (PAC) framework**

$ERR = 1-ACC$

When studying a classification task, we define the **true error** (or **true risk**) as the expected ratio of incorrectly classified instances among all classifications as measured in the deployment distribution $\mathcal{D}$. True error also defines the probability of making an incorrect prediction when sampling a single instance from $\mathcal{D}$. Note that error is the complement of accuracy.

**Empirical error** or **empirical risk**, on the other hand, is the the ratio of erroneous predictions among all instances of a well-defined set (or rather sequence) $S$. If $S$ is drawn from $\mathcal{D}$ and is i.i.d. (independent and identically distributed), then the empirical error converges to the true error as the size of the set tends to infinity (Law of Large Numbers).
